# Evaluating prompting techniques in AI-assisted fact-checking

_Laurence Dierickx & Carl-Gustav Lind√©n, University of Bergen_

Supporting materials for the paper "The less for the more: Evaluating prompting techniques in AI-assisted fact-checking" This repository includes prompt templates, task definitions, and prompt formulations used in a comparative evaluation of large language models (LLMs) for three fact-checking tasks such as summarisation, verdict formulation, and headline generation. These datasets support reproducibility and further research in prompting.
## Tasks:
- **Condensation**: Summarising fact-checks into concise leads.
- **Evaluation**: Providing verdicts based on evidence.
- **Generation**: Creating headlines reflecting the claim and verdict.

### Abstract
Despite the challenges of bias, inaccuracy and hallucinations, large language models (LLMs) are increasingly being used in journalism and fact-checking. However, the potential of prompting strategies to help non-expert users address the issues related to their flaws and limitations has largely been overlooked. This study systematically evaluates seven prompting techniques in three core fact-checking tasks: headline generation, content summarisation, and verdict formulation. The prompts were tested on four popular and publicly accessible LLMs to generate a manually curated dataset of 2,520 outputs simulating real-world practices. The evaluation method consists of a mixed framework that combines human judgements with automated metrics to measure task completion, semantic accuracy, and factual consistency. The results highlight that simple techniques often outperform more sophisticated ones, although structured techniques are beneficial for complex tasks that require reasoning and explainability. This study also highlights the importance of designing clear prompts tailored to specific tasks, insofar as effective prompting does not depend on complexity, but rather on a strategic choice of words and precise task definition. Consequently, this study positions prompting as a core mechanism for trustworthy human-AI collaboration, conceptualising prompting as a form of linguistic engineering, wherein semantic architecture operates as the bridge between human intent and machine interpretation. Such an approach is particularly relevant in scenarios where information quality is critical, making this study both valuable for high-stakes applications and extendable to other domains where information quality is critical.
