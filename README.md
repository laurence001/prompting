# Evaluating prompting strategies for fact-checking

Laurence Dierickx & Carl-Gustav Lind√©n, University of Bergen

Supporting materials for the paper "No Technique Is the Technique: Evaluating Prompting Strategies for Non-Expert Users in Fact-Checking" This repository includes prompt templates, task definitions, and prompt formulations used in a comparative evaluation of large language models (LLMs) for three fact-checking tasks such as summarisation, verdict formulation, and headline generation. These datasets support reproducibility and further research in prompting.
## Tasks:
- **Condensation**: Summarising fact-checks into concise leads.
- **Evaluation**: Providing verdicts based on evidence.
- **Generation**: Creating headlines reflecting the claim and verdict.

### Abstract
Despite ongoing challenges such as bias and hallucinations, large language models (LLMs) are being increasingly integrated into journalistic and fact-checking routines. However, the potential of prompting by non-experts to address these challenges has received little attention. This study systematically evaluates seven prompting techniques for three core fact-checking tasks: headline generation, content summarisation, and verdict formulation. These techniques were tested manually on four publicly and freely accessible LLMs (ChatGPT, Llama, Gemini and Grok) using a curated dataset of fifteen fact-checks, reflecting real-world professional practices. The LLMs' outcomes were assessed using a combination of automated metrics and expert human judgements. The focus was on the models' ability to perform the task and their factual accuracy. The results emphasise the importance of clear, concise and well-defined prompts in improving LLMs' performance in fact-checking tasks and in making effective prompting strategies more accessible to non-experts.


